<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="google-site-verification" content="LYZAMZyp5IGDHDRhRMjN0VoDglk1rEoj9nYv62BRxfQ" />
        <title>snow-dev.com :: ECS CD with CodePipeline in Terraform</title>
        <link rel="stylesheet" href="../css/default.css" />
        <link rel="stylesheet" href="../css/syntax.css" />
        <link rel="stylesheet" href="../css/fontawesome.min.css">
        <link rel="stylesheet" href="../css/fontawesome-solid.min.css">
        <link rel="stylesheet" href="../css/fontawesome-brands.min.css">
    </head>
    <body>
        <header>
            <div class="logo">
                <a href="../">snow-dev.com</a>
            </div>
            <nav>
                <a href="../" title="Home"><i class="fas fa-home"></i></a>
                <a href="../about.html" title="About"><i class="fas fa-info"></i></a>
                <a href="../archive.html" title="Archive"><i class="fas fa-archive"></i></a>
                <a href="../atom.xml" title="Atom Feed"><i class="fas fa-atom"></i></a>
                <a href="../rss.xml" title="RSS Feed"><i class="fas fa-rss"></i></a>
                <a href="https://github.com/snowiow" title="Github"><i class="fab fa-github"></i></a>
                <a href="mailto:marcel.patzwahl@posteo.de" title="Contact Me"><i class="fas fa-envelope"></i></a>
            </nav>
        </header>
          <main role="main">
              <h1>ECS CD with CodePipeline in Terraform</h1>
              <article>
    <section class="header">
        Posted on 2019-05-21
    </section>
    <hr class="section-head">
    <section>
        <p>Last week I came along a problem regarding the deployment of an ECS Service. I wanted to use the <a href="https://www.youtube.com/watch?v=5VPIzKDyLvo">newly announced</a> Blue/Green-Deployment powered by CodeDeploy, because for the time being I only needed one Fargate instance to run. The classic ECS Deployment destroyed one instance and started a new one in it’s place. With one instance this would mean, we would have some downtime during the start of the new container.</p>
<p>With Blue/Green Deployment a completely new group of instances will be started. If everything went fine, the load balancer forwards the traffic to the new group and the old gets shut down afterwards. Even with only one running instance, a second gets started before the first will be destroyed and if the second is working correctly, the traffic switches over and the old will be destroyed afterwards. The traffic switch is a minimal downtime, which isn’t recognizable in normal operation.</p>
<p>In this post we will create a complete Continuous Delivery Pipeline with AWS CodePipeline, which consists of three stages. In the first stage (called source) the pipeline listens to master commits on the GitHub Repository of interest. The second stage (called build) builds a new docker image of the Dockerfile in the GitHub Repository and pushes the new image to AWS ECR. The last state (called deploy) does a blue green deployment to ECS with the new image.</p>
<p>The docker image is a simple HTML website with a message so we can see that the correct container is deployed.</p>
<p>The complete codebase can be found in this <a href="https://github.com/snowiow/green-blue-ecs-example">GitHub Repository</a>.</p>
<p>We start by creating the infrastructure for the ECS Service itself.</p>
<p><img src="../images/ecs-green-blue.png" alt="ECS Green Blue Overview" title="ECS Green Blue Overview"></p>
<p>As you can see we will build up a VPC with three subnets where the ECS Services can be spawned in. The ECS Services have access to the ECR to pull images. In front of our network is the load balancer, which redirects the traffic to the services.</p>
<p>Before we go into the infrastructure, we create a <em>main.tf</em> and create some variables, we need later and create the AWS provider.</p>
<pre class="terraform"><code>variable &quot;github_token&quot; {
  description = &quot;The GitHub Token to be used for the CodePipeline&quot;
  type        = &quot;string&quot;
}

variable &quot;account_id&quot; {
  description = &quot;id of the active account&quot;
  type        = &quot;string&quot;
}

variable &quot;region&quot; {
  description = &quot;region to deploy to&quot;
  type        = &quot;string&quot;
}

provider &quot;aws&quot; {
  region  = &quot;${var.region}&quot;
  version = &quot;2.7&quot;
}</code></pre>
<h1 id="part-1-the-vpc">Part 1: The VPC</h1>
<p>We start with what is the basis of every AWS tutorial: The VPC.</p>
<pre class="terraform"><code>locals {
  subnets = {
    &quot;${var.region}a&quot; = &quot;172.16.0.0/21&quot;
    &quot;${var.region}b&quot; = &quot;172.16.8.0/21&quot;
    &quot;${var.region}c&quot; = &quot;172.16.16.0/21&quot;
  }
}

resource &quot;aws_vpc&quot; &quot;this&quot; {
  cidr_block = &quot;172.16.0.0/16&quot;

  enable_dns_support   = true
  enable_dns_hostnames = true

  tags = {
    Name = &quot;example-vpc&quot;
  }
}

resource &quot;aws_internet_gateway&quot; &quot;this&quot; {
  vpc_id = &quot;${aws_vpc.this.id}&quot;

  tags = {
    Name = &quot;example-internet-gateway&quot;
  }
}

resource &quot;aws_subnet&quot; &quot;this&quot; {
  count      = &quot;${length(local.subnets)}&quot;
  cidr_block = &quot;${element(values(local.subnets), count.index)}&quot;
  vpc_id     = &quot;${aws_vpc.this.id}&quot;

  map_public_ip_on_launch = true
  availability_zone       = &quot;${element(keys(local.subnets), count.index)}&quot;

  tags = {
    Name = &quot;${element(keys(local.subnets), count.index)}&quot;
  }
}

resource &quot;aws_route_table&quot; &quot;this&quot; {
  vpc_id = &quot;${aws_vpc.this.id}&quot;

  tags = {
    Name = &quot;example-route-table-public&quot;
  }
}

resource &quot;aws_route&quot; &quot;this&quot; {
  route_table_id         = &quot;${aws_route_table.this.id}&quot;
  destination_cidr_block = &quot;0.0.0.0/0&quot;
  gateway_id             = &quot;${aws_internet_gateway.this.id}&quot;
}

resource &quot;aws_route_table_association&quot; &quot;this&quot; {
  count          = &quot;${length(local.subnets)}&quot;
  route_table_id = &quot;${aws_route_table.this.id}&quot;
  subnet_id      = &quot;${element(aws_subnet.this.*.id, count.index)}&quot;
}</code></pre>
<p>Here we create a VPC with three subnets in the three AZs of the defined region. The region is a variable which is set in a <em>terraform.tfvars</em> file (at least for me. <a href="https://www.terraform.io/docs/configuration-0-11/variables.html">You can choose other sources for input variables as well</a>). We create a routing table with route associations to the three subnets and a route to an internet gateway. I won’t go into detail about the VPC. For more infos you can read <a href="../posts/vpc-peering-with-mongodb-atlas-and-aws-in-terraform.html">my previous post</a> in which I talk a bit more about VPCs. The difference this time is, that there are three subnets, which split up the whole CIDR space of the VPC. We are making three subnets, because we want to spawn instances randomly in different availability zones. If we increase the count of the running tasks, those will be split evenly over the different AZs. In the rare case of unavailability of one AZ, the app would still be available, because it’s also running in the other AZs.</p>
<h1 id="part-2-the-load-balancer">Part 2: The Load Balancer</h1>
<p>Now we need a load balancer, which is the key part of our Green/Blue Deployment. The Load Balancer has two target groups. We need the two target groups for the green/blue deployment later. That’s why we call one target group green and the other blue. The blue group will stay empty for the time being and we spawn everything in the green one.</p>
<p><img src="../images/ecs-green-blue-alb.png" alt="ECS Green Blue ALB" title="ECS Green Blue ALB"></p>
<pre class="terraform"><code>data &quot;aws_subnet_ids&quot; &quot;this&quot; {
  vpc_id = &quot;${aws_vpc.this.id}&quot;
}

resource &quot;aws_security_group&quot; &quot;this&quot; {
  name   = &quot;allow-http&quot;
  vpc_id = &quot;${aws_vpc.this.id}&quot;

  ingress {
    from_port   = 80
    protocol    = &quot;tcp&quot;
    to_port     = 80
    cidr_blocks = [&quot;0.0.0.0/0&quot;]
  }

  egress {
    from_port   = 0
    protocol    = &quot;-1&quot;
    to_port     = 0
    cidr_blocks = [&quot;0.0.0.0/0&quot;]
  }

  tags {
    Name = &quot;allow-http-sg&quot;
  }
}

resource &quot;aws_lb&quot; &quot;this&quot; {
  name               = &quot;example-lb&quot;
  internal           = false
  load_balancer_type = &quot;application&quot;
  security_groups    = [&quot;${aws_security_group.this.id}&quot;]
  subnets            = [&quot;${data.aws_subnet_ids.this.ids}&quot;]

  tags {
    Name = &quot;example&quot;
  }
}

locals {
  target_groups = [
    &quot;green&quot;,
    &quot;blue&quot;,
  ]
}

resource &quot;aws_lb_target_group&quot; &quot;this&quot; {
  count = &quot;${length(local.target_groups)}&quot;

  name = &quot;example-tg-${element(local.target_groups, count.index)}&quot;

  port        = 80
  protocol    = &quot;HTTP&quot;
  vpc_id      = &quot;${aws_vpc.this.id}&quot;
  target_type = &quot;ip&quot;

  health_check {
    path = &quot;/&quot;
    port = 80
  }
}

resource &quot;aws_lb_listener&quot; &quot;this&quot; {
  load_balancer_arn = &quot;${aws_lb.this.arn}&quot;
  port              = &quot;80&quot;
  protocol          = &quot;HTTP&quot;

  default_action {
    type             = &quot;forward&quot;
    target_group_arn = &quot;${aws_lb_target_group.this.*.arn[0]}&quot;
  }
}

resource &quot;aws_lb_listener_rule&quot; &quot;this&quot; {
  listener_arn = &quot;${aws_lb_listener.this.arn}&quot;

  &quot;action&quot; {
    type             = &quot;forward&quot;
    target_group_arn = &quot;${aws_lb_target_group.this.*.arn[0]}&quot;
  }

  &quot;condition&quot; {
    field  = &quot;path-pattern&quot;
    values = [&quot;/*&quot;]
  }
}</code></pre>
<p>Beside of the before mentioned resources, we need a listener, which will listen on port 80 for this example and a security group. The security group allows incoming traffic via port 80 to the load balancer.</p>
<p>One more thing to point out is the health check on the target groups. Those are important for the blue/green deployment later, because only if the health check succeeds, the group switch will be made. By default the Load Balancer checks the health of the targets every 30 seconds. After 3 consecutive successful checks, the target is healthy and real traffic is forwarded to the target. If a service fails to answer with a 200 status code within 6 seconds three consecutive times, then the target is marked unhealthy and real traffic isn’t forwarded to this target.</p>
<h1 id="part-3-the-ecs-service">Part 3: The ECS Service</h1>
<p>Now we create the ECS Service. AWS has created some concepts on top of the docker container itself. The smallest unit on top of the container in AWS is the <em>task definition</em>. In the task definition, we define what information is passed to one or more containers when they are being run. Basically everything that can be passed as an argument to <em>docker run</em> and some more options can be set in the task definition. For example port mappings or environment variables etc..</p>
<p>Task definitions are normally written in JSON, but there is <a href="https://registry.terraform.io/modules/cloudposse/ecs-container-definition/aws/0.10.0">a module by cloudposse</a> which allows you to write the task definition in terraform. This works similar to the <a href="https://www.terraform.io/docs/providers/aws/d/iam_policy_document.html">aws_iam_policy_document</a>. The advantage of writing those definitions in terraform is to get some more validation, before the resources get applied. Dump mistakes like missing mandatory attributes can be catched early this way.</p>
<p>Here is the basic overview what we are trying to achieve in this part:</p>
<p><img src="../images/ecs_service.png" alt="AWS ECS Service" title="AWS ECS Service"></p>
<p>To use the cloudposses module and create a container definition document we need to import the module:</p>
<pre class="terraform"><code>locals {
  container_name = &quot;green-blue-ecs-example&quot;
}

data &quot;aws_ecr_repository&quot; &quot;this&quot; {
  name = &quot;snowiow/${local.container_name}&quot;
}

resource &quot;aws_cloudwatch_log_group&quot; &quot;this&quot; {
  name = &quot;example-app&quot;
}

module &quot;container_definition&quot; {
  source  = &quot;cloudposse/ecs-container-definition/aws&quot;
  version = &quot;0.13.0&quot;

  container_name  = &quot;${local.container_name}&quot;
  container_image = &quot;${data.aws_ecr_repository.this.repository_url}:latest&quot;

  port_mappings = [
    {
      containerPort = 80
    },
  ]

  log_options = {
    awslogs-region        = &quot;${var.region}&quot;
    awslogs-group         = &quot;example-app&quot;
    awslogs-stream-prefix = &quot;ecs-service&quot;
  }
}</code></pre>
<p>We also define the ECR repository as a data source where will pull the images from. The URL of the latest ECR image will be referenced as the <em>container_image</em> in the container definition. We also create a cloudwatch log group in which the ECS Service can write it’s logs. Therefore we also define the according log options in the container definition.</p>
<p>Because we are just hosting a simple HTML website, there isn’t much more going on in the container definition.</p>
<p>Now we need two roles. One which is used by the task, which is able to run the task definition and the actual rights of the container during runtime. Those are the so called <em>task role</em> and <em>execution role</em>.</p>
<pre class="terraform"><code>resource &quot;aws_ecs_cluster&quot; &quot;this&quot; {
  name = &quot;example-cluster&quot;
}

data &quot;aws_iam_policy_document&quot; &quot;assume_by_ecs&quot; {
  statement {
    sid     = &quot;AllowAssumeByEcsTasks&quot;
    effect  = &quot;Allow&quot;
    actions = [&quot;sts:AssumeRole&quot;]

    principals {
      type        = &quot;Service&quot;
      identifiers = [&quot;ecs-tasks.amazonaws.com&quot;]
    }
  }
}

data &quot;aws_iam_policy_document&quot; &quot;execution_role&quot; {
  statement {
    sid    = &quot;AllowECRPull&quot;
    effect = &quot;Allow&quot;

    actions = [
      &quot;ecr:GetDownloadUrlForLayer&quot;,
      &quot;ecr:BatchGetImage&quot;,
      &quot;ecr:BatchCheckLayerAvailability&quot;,
    ]

    resources = [&quot;${data.aws_ecr_repository.this.arn}&quot;]
  }

  statement {
    sid    = &quot;AllowECRAuth&quot;
    effect = &quot;Allow&quot;

    actions = [&quot;ecr:GetAuthorizationToken&quot;]

    resources = [&quot;*&quot;]
  }

  statement {
    sid    = &quot;AllowLogging&quot;
    effect = &quot;Allow&quot;

    actions = [
      &quot;logs:CreateLogStream&quot;,
      &quot;logs:PutLogEvents&quot;,
    ]

    resources = [&quot;*&quot;]
  }
}

data &quot;aws_iam_policy_document&quot; &quot;task_role&quot; {
  statement {
    sid    = &quot;AllowDescribeCluster&quot;
    effect = &quot;Allow&quot;

    actions = [&quot;ecs:DescribeClusters&quot;]

    resources = [&quot;${aws_ecs_cluster.this.arn}&quot;]
  }
}

resource &quot;aws_iam_role&quot; &quot;execution_role&quot; {
  name               = &quot;ecs-example-execution-role&quot;
  assume_role_policy = &quot;${data.aws_iam_policy_document.assume_by_ecs.json}&quot;
}

resource &quot;aws_iam_role_policy&quot; &quot;execution_role&quot; {
  role   = &quot;${aws_iam_role.execution_role.name}&quot;
  policy = &quot;${data.aws_iam_policy_document.execution_role.json}&quot;
}

resource &quot;aws_iam_role&quot; &quot;task_role&quot; {
  name               = &quot;ecs-example-task-role&quot;
  assume_role_policy = &quot;${data.aws_iam_policy_document.assume_by_ecs.json}&quot;
}

resource &quot;aws_iam_role_policy&quot; &quot;task_role&quot; {
  role   = &quot;${aws_iam_role.task_role.name}&quot;
  policy = &quot;${data.aws_iam_policy_document.task_role.json}&quot;
}</code></pre>
<p>First of all we create the ECS Cluster, because we want to reference it as the only resource for the task role policy document later.</p>
<p>Afterwards we need an assume role policy document, which can be assumed by ECS Tasks. This document will be attached to both the task role and execution role. So both can be assumed by our task, which will be created next.</p>
<p>Now we define the permissions of the execution role and task role. It is important that we are able to download images from the ECR during the task execution, as well as writing logs, so we pass the needed permissions to the document and restrict the ECR download actions to the ECR Repository of the example app. The task role needs permissions to describe the cluster it can be run in.</p>
<p>Lastly we create the execution role and task role and append the policy documents to those roles.</p>
<p>Now we have everything in place to create the task definition and service:</p>
<pre class="terraform"><code>resource &quot;aws_ecs_task_definition&quot; &quot;this&quot; {
  family                   = &quot;green-blue-ecs-example&quot;
  container_definitions    = &quot;${module.container_definition.json}&quot;
  execution_role_arn       = &quot;${aws_iam_role.execution_role.arn}&quot;
  task_role_arn            = &quot;${aws_iam_role.task_role.arn}&quot;
  network_mode             = &quot;awsvpc&quot;
  cpu                      = &quot;0.25 vcpu&quot;
  memory                   = &quot;0.5 gb&quot;
  requires_compatibilities = [&quot;FARGATE&quot;]
}

resource &quot;aws_security_group&quot; &quot;ecs&quot; {
  name   = &quot;allow-ecs-traffic&quot;
  vpc_id = &quot;${aws_vpc.this.id}&quot;

  ingress {
    from_port   = 80
    protocol    = &quot;tcp&quot;
    to_port     = 443
    cidr_blocks = [&quot;0.0.0.0/0&quot;]
  }

  egress {
    from_port   = 0
    protocol    = &quot;-1&quot;
    to_port     = 0
    cidr_blocks = [&quot;0.0.0.0/0&quot;]
  }
}

resource &quot;aws_ecs_service&quot; &quot;this&quot; {
  name            = &quot;example-service&quot;
  task_definition = &quot;${aws_ecs_task_definition.this.id}&quot;
  cluster         = &quot;${aws_ecs_cluster.this.arn}&quot;

  load_balancer {
    target_group_arn = &quot;${aws_lb_target_group.this.0.arn}&quot;
    container_name   = &quot;${local.container_name}&quot;
    container_port   = 80
  }

  launch_type   = &quot;FARGATE&quot;
  desired_count = 3

  network_configuration {
    subnets         = [&quot;${aws_subnet.this.*.id}&quot;]
    security_groups = [&quot;${aws_security_group.ecs.id}&quot;]

    assign_public_ip = true
  }

  depends_on = [&quot;aws_lb_listener.this&quot;]
}</code></pre>
<p>For the task definition we don’t set anything special, except that we force the awsvpc network mode to spawn it in our earlier created VPC. In the service we define a load balancer block. This way any task started by this service will be spawned in the target group of our load balancer. Here we start to fill the green group initially. We also have to set the subnets, where the service spawns the tasks in. The service has a desired task count of three. AWS balances these three tasks in all of our subnets evenly, so we will have one task running in every subnet.</p>
<p>As the security group we use one with allowed ingress from port 80 to 443. The normal traffic from the load balancer comes from port 80, but we also need to be able to download the docker image from the ECR, which will be done via HTTPS. We also need to assign a public IP to the task to be able to download the image.</p>
<p>It is important to note, that we need to explicitly name a dependency to the <em>aws_lb_listener</em>. Because we apply anything simultaneously, the service would otherwise just wait for the target group to be finished. But then the creation of the ECS Service would give us an error, because the created target group isn’t linked to a load balancer. That’s why we need to explicitly wait for the listener to be created before we start to create the ECS Service.</p>
<p>At this point we have a fully working infrastructure to host our dockerized application, which can be scaled by modifying the cpu and memory attributes and a new <em>terraform apply</em>. We can also add an Autoscaling Policy to scale our task count automatically when the traffic increases.</p>
<h1 id="part-4-the-deployment">Part 4: The Deployment</h1>
<p>Now that the application is up and running, we need some sort of automatic building and deploying. Therefore we use the AWS Codepipeline, which will consist of three steps:</p>
<ol type="1">
<li>Source: Trigger the pipeline through a master commit in the GitHub repository of the application</li>
<li>Build: Build a new container and push it to the ECR</li>
<li>Deploy: Do a Blue/Green Deployment in our ECS Service with the latest container version</li>
</ol>
<p>We start by creating the pipeline with it’s first stage. Therefore we need a new IAM Role for the pipeline and a S3 Bucket, where the interim results of the pipeline are saved and downloaded by the next step of the pipeline. The S3 Bucket is very straight forward:</p>
<pre class="terraform"><code>resource &quot;aws_s3_bucket&quot; &quot;codepipeline&quot; {
  bucket = &quot;example-app-codepipeline&quot;
}</code></pre>
<p>and the iam role:</p>
<pre class="terraform"><code>data &quot;aws_iam_policy_document&quot; &quot;assume_by_pipeline&quot; {
  statement {
    sid     = &quot;AllowAssumeByPipeline&quot;
    effect  = &quot;Allow&quot;
    actions = [&quot;sts:AssumeRole&quot;]

    principals {
      type        = &quot;Service&quot;
      identifiers = [&quot;codepipeline.amazonaws.com&quot;]
    }
  }
}

resource &quot;aws_iam_role&quot; &quot;pipeline&quot; {
  name               = &quot;pipeline-example-role&quot;
  assume_role_policy = &quot;${data.aws_iam_policy_document.assume_by_pipeline.json}&quot;
}

data &quot;aws_iam_policy_document&quot; &quot;pipeline&quot; {
  statement {
    sid    = &quot;AllowS3&quot;
    effect = &quot;Allow&quot;

    actions = [
      &quot;s3:GetObject&quot;,
      &quot;s3:ListBucket&quot;,
      &quot;s3:PutObject&quot;,
    ]

    resources = [
      &quot;${aws_s3_bucket.this.arn}&quot;,
      &quot;${aws_s3_bucket.this.arn}/*&quot;,
    ]
  }
}

resource &quot;aws_iam_role_policy&quot; &quot;pipeline&quot; {
  role   = &quot;${aws_iam_role.pipeline.name}&quot;
  policy = &quot;${data.aws_iam_policy_document.pipeline.json}&quot;
}</code></pre>
<p>For the moment we only give the pipeline the permissions to list, download and upload to the bucket, we created before.</p>
<p>Now we can create the pipeline.</p>
<pre class="terraform"><code>resource &quot;aws_codepipeline&quot; &quot;this&quot; {
  name     = &quot;example-pipeline&quot;
  role_arn = &quot;${aws_iam_role.pipeline.arn}&quot;

  artifact_store {
    location = &quot;${aws_s3_bucket.this.bucket}&quot;
    type     = &quot;S3&quot;
  }

  stage {
    name = &quot;Source&quot;

    action {
      name             = &quot;Source&quot;
      category         = &quot;Source&quot;
      owner            = &quot;ThirdParty&quot;
      provider         = &quot;GitHub&quot;
      version          = &quot;1&quot;
      output_artifacts = [&quot;source&quot;]

      configuration = {
        OAuthToken = &quot;${var.github_token}&quot;
        Owner      = &quot;snowiow&quot;
        Repo       = &quot;green-blue-ecs-example&quot;
        Branch     = &quot;master&quot;
      }
    }
  }
}</code></pre>
<p>We define GitHub as the source and my repository’s master branch as the start trigger. You will need an OAuthToken as well. This way the pipeline will be notified when a new commit happened. The other way would be to let the pipeline pull information from GitHub every now and then. But with this option you will always have some time between the commit and the start of the pipeline. To generate a GitHub Token with the right scopes, you can follow <a href="https://docs.aws.amazon.com/codepipeline/latest/userguide/GitHub-rotate-personal-token-CLI.html">this guide from AWS</a>. If you copied the token you can use it for example as a variable in terraform. Another way is to set the <em>GITHUB_TOKEN</em> environment variable. This way you can leave the OAuthToken attribute unset.</p>
<p>Now we will add the build stage. First of all, the CodeBuild needs it’s own set of permissions, so we do the usual stuff to create an IAM role for the CodeBuild:</p>
<pre class="terraform"><code>data &quot;aws_iam_policy_document&quot; &quot;assume_by_codebuild&quot; {
  statement {
    sid     = &quot;AllowAssumeByCodebuild&quot;
    effect  = &quot;Allow&quot;
    actions = [&quot;sts:AssumeRole&quot;]

    principals {
      type        = &quot;Service&quot;
      identifiers = [&quot;codebuild.amazonaws.com&quot;]
    }
  }
}

resource &quot;aws_iam_role&quot; &quot;codebuild&quot; {
  name               = &quot;codebuild-example-role&quot;
  assume_role_policy = &quot;${data.aws_iam_policy_document.assume_by_codebuild.json}&quot;
}

data &quot;aws_iam_policy_document&quot; &quot;codebuild&quot; {
  statement {
    sid    = &quot;AllowS3&quot;
    effect = &quot;Allow&quot;

    actions = [
      &quot;s3:GetObject&quot;,
      &quot;s3:ListBucket&quot;,
      &quot;s3:PutObject&quot;,
    ]

    resources = [
      &quot;${aws_s3_bucket.this.arn}&quot;,
      &quot;${aws_s3_bucket.this.arn}/*&quot;,
    ]
  }

  statement {
    sid    = &quot;AllowECRAuth&quot;
    effect = &quot;Allow&quot;

    actions = [&quot;ecr:GetAuthorizationToken&quot;]

    resources = [&quot;*&quot;]
  }

  statement {
    sid    = &quot;AllowECRUpload&quot;
    effect = &quot;Allow&quot;

    actions = [
      &quot;ecr:InitiateLayerUpload&quot;,
      &quot;ecr:UploadLayerPart&quot;,
      &quot;ecr:CompleteLayerUpload&quot;,
      &quot;ecr:BatchCheckLayerAvailability&quot;,
      &quot;ecr:PutImage&quot;,
    ]

    resources = [&quot;${data.aws_ecr_repository.this.arn}&quot;]
  }

  statement {
    sid       = &quot;AllowECSDescribeTaskDefinition&quot;
    effect    = &quot;Allow&quot;
    actions   = [&quot;ecs:DescribeTaskDefinition&quot;]
    resources = [&quot;*&quot;]
  }

  statement {
    sid    = &quot;AllowLogging&quot;
    effect = &quot;Allow&quot;

    actions = [
      &quot;logs:CreateLogGroup&quot;,
      &quot;logs:CreateLogStream&quot;,
      &quot;logs:PutLogEvents&quot;,
    ]

    resources = [&quot;*&quot;]
  }
}

resource &quot;aws_iam_role_policy&quot; &quot;codebuild&quot; {
  role   = &quot;${aws_iam_role.codebuild.name}&quot;
  policy = &quot;${data.aws_iam_policy_document.codebuild.json}&quot;
}</code></pre>
<p>Because our CodeBuild should be able to upload Images to the ECR, we give the according permissions here. The CodeBuild also needs permissions to access the S3 Bucket, to download the artifact from the Source (GitHub). Otherwise the Codebuild wouldn’t be able to access the downloaded source code of GitHub and therefeore couldn’t create the Docker Image.</p>
<p>Now we are able to create the Codebuild project like this:</p>
<pre class="terraform"><code>resource &quot;aws_codebuild_project&quot; &quot;this&quot; {
  name         = &quot;example-codebuild&quot;
  description  = &quot;Codebuild for the ECS Green/Blue Example app&quot;
  service_role = &quot;${aws_iam_role.codebuild.arn}&quot;

  artifacts {
    type = &quot;CODEPIPELINE&quot;
  }

  environment {
    compute_type    = &quot;BUILD_GENERAL1_SMALL&quot;
    image           = &quot;aws/codebuild/docker:18.09.0&quot;
    type            = &quot;LINUX_CONTAINER&quot;
    privileged_mode = true

    environment_variable {
      name = &quot;REPOSITORY_URI&quot;
      value = &quot;${data.aws_ecr_repository.this.repository_url}&quot;
    }
  }
  source {
    type = &quot;CODEPIPELINE&quot;
  }
}</code></pre>
<p>Nothing special here. We append the IAM role, which we created earlier to the CodeBuild Project. The Environment settings are the information for which kind of machine we will execute the build. As the source we define <em>CODEPIPELINE</em>. Because the artifacts from the source stage is already provided there. That’s why we need to set <em>CODEPIPELINE</em> as the artifacts type as well.</p>
<p>The Codebuild will be appended to the CodePipeline as an additional stage:</p>
<pre class="terraform"><code>stage {
  name = &quot;Build&quot;

  action {
    name             = &quot;Build&quot;
    category         = &quot;Build&quot;
    owner            = &quot;AWS&quot;
    provider         = &quot;CodeBuild&quot;
    version          = &quot;1&quot;
    input_artifacts  = [&quot;source&quot;]
    output_artifacts = [&quot;build&quot;]

    configuration {
      ProjectName = &quot;{aws_codebuild_project.this.name}&quot;
    }
  }
}</code></pre>
<p>Here you can see how we connect the artifact inputs and outputs of the different stages. In the source stage we created an <em>output_artifact</em>, which is used as an <em>input_artifact</em> in the build stage. The artifact names are subdirectories of the S3 Bucket we defined earlier. In these directories the artifact is saved as a zip file with a specific hash identifying the build, it belongs to.</p>
<p>Because the CodeBuild will be triggered from the CodePipeline, we need to give it the corresponding permissions. Therefore we add the following statement to the CodePipeline Permissions:</p>
<pre class="terraform"><code>statement {
  sid    = &quot;AllowCodeBuild&quot;
  effect = &quot;Allow&quot;

  actions = [
    &quot;codebuild:BatchGetBuilds&quot;,
    &quot;codebuild:StartBuild&quot;,
  ]

  resources = [&quot;${aws_codebuild_project.this.arn}&quot;]
}</code></pre>
<p>Now we have everything in place to run builds. But we haven’t yet defined what the build should actually do. CodeBuild works similar to other CI/CD Services like Travis and CircleCI. We need to place a file in the root directory, which describes the build steps. In CodeBuild this file is called <em>buildspec.yml</em> an is written in YAML. My <em>buildspec.yml</em> looks like this:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode yaml"><code class="sourceCode yaml"><a class="sourceLine" id="cb14-1" title="1"><span class="fu">version:</span><span class="at"> </span><span class="fl">0.2</span></a>
<a class="sourceLine" id="cb14-2" title="2"></a>
<a class="sourceLine" id="cb14-3" title="3"><span class="fu">phases:</span></a>
<a class="sourceLine" id="cb14-4" title="4">  <span class="fu">pre_build:</span></a>
<a class="sourceLine" id="cb14-5" title="5">    <span class="fu">commands:</span></a>
<a class="sourceLine" id="cb14-6" title="6">      <span class="kw">-</span> echo Logging in to Amazon ECR...</a>
<a class="sourceLine" id="cb14-7" title="7">      <span class="kw">-</span> $(aws ecr get-login --region eu-central-1 --no-include-email)</a>
<a class="sourceLine" id="cb14-8" title="8">      <span class="kw">-</span> IMAGE_TAG=$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | cut -c 1-7)</a>
<a class="sourceLine" id="cb14-9" title="9">  <span class="fu">build:</span></a>
<a class="sourceLine" id="cb14-10" title="10">    <span class="fu">commands:</span></a>
<a class="sourceLine" id="cb14-11" title="11">      <span class="kw">-</span> echo Build started on `date`</a>
<a class="sourceLine" id="cb14-12" title="12">      <span class="kw">-</span> echo Building the Docker image...</a>
<a class="sourceLine" id="cb14-13" title="13">      <span class="kw">-</span> docker build -t $REPOSITORY_URI:latest .</a>
<a class="sourceLine" id="cb14-14" title="14">      <span class="kw">-</span> docker tag $REPOSITORY_URI:latest $REPOSITORY_URI:$IMAGE_TAG</a>
<a class="sourceLine" id="cb14-15" title="15">  <span class="fu">post_build:</span></a>
<a class="sourceLine" id="cb14-16" title="16">    <span class="fu">commands:</span></a>
<a class="sourceLine" id="cb14-17" title="17">      <span class="kw">-</span> echo Build completed on `date`</a>
<a class="sourceLine" id="cb14-18" title="18">      <span class="kw">-</span> echo Pushing the Docker images...</a>
<a class="sourceLine" id="cb14-19" title="19">      <span class="kw">-</span> docker push $REPOSITORY_URI:latest</a>
<a class="sourceLine" id="cb14-20" title="20">      <span class="kw">-</span> docker push $REPOSITORY_URI:$IMAGE_TAG</a></code></pre></div>
<p>The build consists of three phases. In the <em>pre_build</em> the build container is logging into the ECR and I set some environment variables, which I use during the build. The build phase builds the docker image from the <em>Dockerfile</em> of the root directory. In the <em>post_build</em> the image is pushed to the ECR.</p>
<p>This file will be placed in the project root. Because the source stage of the pipeline downloads the whole repository from github and set it as an input artifact for the build, CodeBuild looks automatically for a <em>buildspec.yml</em> there.</p>
<p>Now we conclude with the CodeDeploy. Like the <em>buildspec.yml</em> for the CodeBuild, there is a <em>appspec.yaml</em> for CodeDeploy (Yes, here the suffix is yaml, not yml by default). This file will be placed in the directory root as well and holds some environment variables, which will be set during the build:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode yaml"><code class="sourceCode yaml"><a class="sourceLine" id="cb15-1" title="1"><span class="fu">version:</span><span class="at"> </span><span class="fl">0.0</span></a>
<a class="sourceLine" id="cb15-2" title="2"><span class="fu">Resources:</span></a>
<a class="sourceLine" id="cb15-3" title="3">  <span class="kw">-</span> <span class="fu">TargetService:</span></a>
<a class="sourceLine" id="cb15-4" title="4">      <span class="fu">Type:</span><span class="at"> AWS::ECS::Service</span></a>
<a class="sourceLine" id="cb15-5" title="5">      <span class="fu">Properties:</span></a>
<a class="sourceLine" id="cb15-6" title="6">        <span class="fu">TaskDefinition:</span><span class="at"> </span><span class="st">&quot;$TASK_DEFINITION&quot;</span></a>
<a class="sourceLine" id="cb15-7" title="7">        <span class="fu">LoadBalancerInfo:</span></a>
<a class="sourceLine" id="cb15-8" title="8">          <span class="fu">ContainerName:</span><span class="at"> </span><span class="st">&quot;$CONTAINER_NAME&quot;</span></a>
<a class="sourceLine" id="cb15-9" title="9">          <span class="fu">ContainerPort:</span><span class="at"> </span><span class="st">&quot;80&quot;</span></a>
<a class="sourceLine" id="cb15-10" title="10">        <span class="fu">PlatformVersion:</span><span class="at"> </span><span class="st">&quot;LATEST&quot;</span></a>
<a class="sourceLine" id="cb15-11" title="11">        <span class="fu">NetworkConfiguration:</span></a>
<a class="sourceLine" id="cb15-12" title="12">          <span class="fu">AwsvpcConfiguration:</span></a>
<a class="sourceLine" id="cb15-13" title="13">            <span class="fu">Subnets:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;$SUBNET_1&quot;</span><span class="kw">,</span><span class="st">&quot;$SUBNET_2&quot;</span><span class="kw">,</span><span class="st">&quot;$SUBNET_3&quot;</span><span class="kw">]</span></a>
<a class="sourceLine" id="cb15-14" title="14">            <span class="fu">SecurityGroups:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;$SECURITY_GROUP&quot;</span><span class="kw">]</span></a>
<a class="sourceLine" id="cb15-15" title="15">            <span class="fu">AssignPublicIp:</span><span class="at"> </span><span class="st">&quot;ENABLED&quot;</span></a></code></pre></div>
<p>This file is saved as <em>appspec_template.yaml</em> in the directory root. In the <em>buildspec.yml</em> we add an install phase at the beginning, which installs jq:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode yaml"><code class="sourceCode yaml"><a class="sourceLine" id="cb16-1" title="1"><span class="fu">install:</span></a>
<a class="sourceLine" id="cb16-2" title="2">  <span class="fu">commands:</span></a>
<a class="sourceLine" id="cb16-3" title="3">    <span class="kw">-</span> apt-get update</a>
<a class="sourceLine" id="cb16-4" title="4">    <span class="kw">-</span> apt install jq</a></code></pre></div>
<p>We need jq to get the current task definition and pass it to codedeploy. This is the second file which is needed by CodeDeploy next to the appspec.yml. In the post build section we save the most current task definition as a file, which will be passed as an artifact to CodeDeploy:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode yaml"><code class="sourceCode yaml"><a class="sourceLine" id="cb17-1" title="1"><span class="kw">-</span> aws ecs describe-task-definition --task-definition green-blue-ecs-example | \</a>
<a class="sourceLine" id="cb17-2" title="2">  jq <span class="st">'.taskDefinition'</span> &gt; taskdef.json</a></code></pre></div>
<p>With jq we extract the actual task definition and write it to a file.</p>
<p>Now we substitute every environment variable in the appspec_template.yaml with <em>envsubst</em> and write it to the actual <em>appspec.yml</em>:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode yaml"><code class="sourceCode yaml"><a class="sourceLine" id="cb18-1" title="1"><span class="kw">-</span> envsubst &lt; appspec_template.yaml &gt; appspec.yaml</a></code></pre></div>
<p>To actually make the substitution work, we need to set the needed environment variables. We set them in terraform in the <em>aws_codebuild_project</em> resource. So the final resource looks like this:</p>
<pre class="terraform"><code>resource &quot;aws_codebuild_project&quot; &quot;this&quot; {
  name         = &quot;example-codebuild&quot;
  description  = &quot;Codebuild for the ECS Green/Blue Example app&quot;
  service_role = &quot;${aws_iam_role.codebuild.arn}&quot;

  artifacts {
    type = &quot;CODEPIPELINE&quot;
  }

  environment {
    compute_type    = &quot;BUILD_GENERAL1_SMALL&quot;
    image           = &quot;aws/codebuild/docker:18.09.0&quot;
    type            = &quot;LINUX_CONTAINER&quot;
    privileged_mode = true

    environment_variable {
      name  = &quot;REPOSITORY_URI&quot;
      value = &quot;${data.aws_ecr_repository.this.repository_url}&quot;
    }

    environment_variable {
      name  = &quot;TASK_DEFINITION&quot;
      value = &quot;arn:aws:ecs:${var.region}:${var.account_id}:task-definition/${aws_ecs_task_definition.this.family}&quot;
    }

    environment_variable {
      name  = &quot;CONTAINER_NAME&quot;
      value = &quot;${local.container_name}&quot;
    }

    environment_variable {
      name  = &quot;SUBNET_1&quot;
      value = &quot;${aws_subnet.this.*.id[0]}&quot;
    }

    environment_variable {
      name  = &quot;SUBNET_2&quot;
      value = &quot;${aws_subnet.this.*.id[1]}&quot;
    }

    environment_variable {
      name  = &quot;SUBNET_3&quot;
      value = &quot;${aws_subnet.this.*.id[2]}&quot;
    }

    environment_variable {
      name  = &quot;SECURITY_GROUP&quot;
      value = &quot;${aws_security_group.ecs.id}&quot;
    }
  }

  source {
    type = &quot;CODEPIPELINE&quot;
  }
}</code></pre>
<p>It is important to note, that we give the task definition arn wihtout it’s revision. This is how we always force, that the most current task definition revision is used.</p>
<p>To pass these two files to CodeDeploy, we set them as artifacts of the CodeBuild in the <em>buildspec.yml</em>:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode yaml"><code class="sourceCode yaml"><a class="sourceLine" id="cb20-1" title="1"><span class="fu">artifacts:</span></a>
<a class="sourceLine" id="cb20-2" title="2">  <span class="fu">files:</span></a>
<a class="sourceLine" id="cb20-3" title="3">    <span class="kw">-</span> appspec.yaml</a>
<a class="sourceLine" id="cb20-4" title="4">    <span class="kw">-</span> taskdef.json</a></code></pre></div>
<p>Because we already set the output artifacts path in the <em>aws_codepipeline</em> resource, these files will be saved in the build directory in S3, which will be picked up by CodeDeploy afterwards.</p>
<p>Now we need the IAM Policy for CodeDeploy:</p>
<pre class="terraform"><code>data &quot;aws_iam_policy_document&quot; &quot;assume_by_codedeploy&quot; {
  statement {
    sid     = &quot;&quot;
    effect  = &quot;Allow&quot;
    actions = [&quot;sts:AssumeRole&quot;]

    principals {
      type        = &quot;Service&quot;
      identifiers = [&quot;codedeploy.amazonaws.com&quot;]
    }
  }
}

resource &quot;aws_iam_role&quot; &quot;codedeploy&quot; {
  name               = &quot;codedeploy&quot;
  assume_role_policy = &quot;${data.aws_iam_policy_document.assume_by_codedeploy.json}&quot;
}

data &quot;aws_iam_policy_document&quot; &quot;codedeploy&quot; {
  statement {
    sid    = &quot;AllowLoadBalancingAndECSModifications&quot;
    effect = &quot;Allow&quot;

    actions = [
      &quot;ecs:CreateTaskSet&quot;,
      &quot;ecs:DeleteTaskSet&quot;,
      &quot;ecs:DescribeServices&quot;,
      &quot;ecs:UpdateServicePrimaryTaskSet&quot;,
      &quot;elasticloadbalancing:DescribeListeners&quot;,
      &quot;elasticloadbalancing:DescribeRules&quot;,
      &quot;elasticloadbalancing:DescribeTargetGroups&quot;,
      &quot;elasticloadbalancing:ModifyListener&quot;,
      &quot;elasticloadbalancing:ModifyRule&quot;,
    ]

    resources = [&quot;*&quot;]
  }

  statement {
    sid    = &quot;AllowS3&quot;
    effect = &quot;Allow&quot;

    actions = [&quot;s3:GetObject&quot;]

    resources = [&quot;${aws_s3_bucket.this.arn}/*&quot;]
  }

  statement {
    sid    = &quot;AllowPassRole&quot;
    effect = &quot;Allow&quot;

    actions = [&quot;iam:PassRole&quot;]

    resources = [
      &quot;${aws_iam_role.execution_role.arn}&quot;,
      &quot;${aws_iam_role.task_role.arn}&quot;,
    ]
  }
}

resource &quot;aws_iam_role_policy&quot; &quot;codedeploy&quot; {
  role   = &quot;${aws_iam_role.codedeploy.name}&quot;
  policy = &quot;${data.aws_iam_policy_document.codedeploy.json}&quot;
}</code></pre>
<p>Everything is analogous to the CodeBuild and the pipeline policies. CodeDeploy needs permissions to modify the ECS task sets and the load balancer. But CodeDeploy just needs to get the items from S3 and doesn’t write anything back.</p>
<p>Now we can create the CodeDeploy App and the Deployment Group:</p>
<pre class="terraform"><code>resource &quot;aws_codedeploy_deployment_group&quot; &quot;this&quot; {
  app_name               = &quot;${aws_codedeploy_app.this.name}&quot;
  deployment_group_name  = &quot;example-deploy-group&quot;
  deployment_config_name = &quot;CodeDeployDefault.ECSAllAtOnce&quot;
  service_role_arn       = &quot;${aws_iam_role.codedeploy.arn}&quot;

  blue_green_deployment_config {
    deployment_ready_option {
      action_on_timeout = &quot;CONTINUE_DEPLOYMENT&quot;
    }

    terminate_blue_instances_on_deployment_success {
      action = &quot;TERMINATE&quot;
    }
  }

  ecs_service {
    cluster_name = &quot;${aws_ecs_cluster.this.name}&quot;
    service_name = &quot;${aws_ecs_service.this.name}&quot;
  }

  deployment_style {
    deployment_option = &quot;WITH_TRAFFIC_CONTROL&quot;
    deployment_type   = &quot;BLUE_GREEN&quot;
  }

  load_balancer_info {
    target_group_pair_info {
      prod_traffic_route {
        listener_arns = [&quot;${aws_lb_listener.this.arn}&quot;]
      }

      target_group {
        name = &quot;${aws_lb_target_group.this.*.name[0]}&quot;
      }

      target_group {
        name = &quot;${aws_lb_target_group.this.*.name[1]}&quot;
      }
    }
  }
}</code></pre>
<p>Here we define the deployment group to be a blue/green deployment. We define our ECS Service to be the Service where to deploy in. We also give it the load blancer listener and the target groups. With these infos AWS is now able to create a fully working blue/green deployment.</p>
<p>The last step is to add the deployment to the pipeline as a new stage:</p>
<pre class="terraform"><code>stage {
  name = &quot;Deploy&quot;

  action {
    name            = &quot;Deploy&quot;
    category        = &quot;Deploy&quot;
    owner           = &quot;AWS&quot;
    provider        = &quot;CodeDeployToECS&quot;
    version         = &quot;1&quot;
    input_artifacts = [&quot;build&quot;]

    configuration {
      ApplicationName                = &quot;${aws_codedeploy_app.this.name}&quot;
      DeploymentGroupName            = &quot;${aws_codedeploy_deployment_group.this.deployment_group_name}&quot;
      TaskDefinitionTemplateArtifact = &quot;build&quot;
      AppSpecTemplateArtifact        = &quot;build&quot;
    }
  }
}</code></pre>
<p>And we need to extend the IAM permissions, so the CodePipeline is able to to trigger CodeDeploy:</p>
<pre class="terraform"><code>statement {
  sid    = &quot;AllowCodeDeploy&quot;
  effect = &quot;Allow&quot;

  actions = [
    &quot;codedeploy:CreateDeployment&quot;,
    &quot;codedeploy:GetApplication&quot;,
    &quot;codedeploy:GetApplicationRevision&quot;,
    &quot;codedeploy:GetDeployment&quot;,
    &quot;codedeploy:GetDeploymentConfig&quot;,
    &quot;codedeploy:RegisterApplicationRevision&quot;,
  ]

  resources = [&quot;*&quot;]

  statement {
    sid    = &quot;AllowECS&quot;
    effect = &quot;Allow&quot;

    actions = [&quot;ecs:*&quot;]

    resources = [&quot;*&quot;]
  }

  statement {
    sid    = &quot;AllowPassRole&quot;
    effect = &quot;Allow&quot;

    resources = [&quot;*&quot;]

    actions = [&quot;iam:PassRole&quot;]

    condition {
      test     = &quot;StringLike&quot;
      values   = [&quot;ecs-tasks.amazonaws.com&quot;]
      variable = &quot;iam:PassedToService&quot;
    }
  }
}</code></pre>
<p>To make an ECS Service deployable by CodeDeploy, we add the following block to the <em>aws_ecs_service</em> resource:</p>
<pre class="terraform"><code>deployment_controller {
  type = &quot;CODE_DEPLOY&quot;
}</code></pre>
<p>With everything applied, a first AWS CodePipeline run through is triggered automatically. The progress of the pipeline can be followed in the AWS Web Console:</p>
<p><img src="../images/codepipeline.png" alt="AWS Codepipeline" title="AWS Codepipeline"></p>
<p>CodeDeploy also has a very good overview of the state of the Blue/Green Deployment, where you can see how much traffic to which target group is forwarded at the moment:</p>
<p><img src="../images/codedeploy.png" alt="AWS Codedeploy Traffic" title="AWS Codedeploy Traffic"></p>
<p>Furthermore you can append validation lambdas to different states of the deployment to check automatically, if the newly started version of your application is working correctly. For example you can program a lambda which checks your newly created application routes for a 500 status code. When the lambda discovers one, it returns a validation failure and the deployment is rolled back automatically. You get an overview of the lifecycle events further down the page.</p>
<p><img src="../images/codedeploy2.png" alt="AWS Codedeploy Lifecycle" title="AWS Codedeploy Lifecycle"></p>
<p>I hope this post was helpful. It is very extensive and some parts, where it could be shortened. Especially the explicit IAM permissions take up a big part. But I don’t want to encourage you to use the predefined policies, because they often allow more than needed. If you are writing your own policies you get much more control and can fine grain your permissions for every AWS service you instantiate.</p>
    </section>
</article>

          </main>
          <footer>
            <nav>
                <a href="../impressum.html" title="Impressum">Impressum</a>
                <a href="../datenschutz.html" title="Datenschutz">Datenschutz</a>
            </nav>
          </footer>
    </body>
</html>
