<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="google-site-verification" content="LYZAMZyp5IGDHDRhRMjN0VoDglk1rEoj9nYv62BRxfQ" />
        <title>snow-dev.com :: VPC Peering with MongoDB Atlas and AWS in Terraform</title>
        <link rel="stylesheet" href="../css/default.css" />
        <link rel="stylesheet" href="../css/syntax.css" />
        <link rel="stylesheet" href="../css/fontawesome.min.css">
        <link rel="stylesheet" href="../css/fontawesome-solid.min.css">
        <link rel="stylesheet" href="../css/fontawesome-brands.min.css">
    </head>
    <body>
        <header>
            <div class="logo">
                <a href="../">snow-dev.com</a>
            </div>
            <nav>
                <a href="../" title="Home"><i class="fas fa-home"></i></a>
                <a href="../about.html" title="About"><i class="fas fa-info"></i></a>
                <a href="../archive.html" title="Archive"><i class="fas fa-archive"></i></a>
                <a href="../atom.xml" title="Atom Feed"><i class="fas fa-atom"></i></a>
                <a href="../rss.xml" title="RSS Feed"><i class="fas fa-rss"></i></a>
                <a href="https://github.com/snowiow" title="Github"><i class="fab fa-github"></i></a>
                <a href="mailto:marcel.patzwahl@posteo.de" title="Contact Me"><i class="fas fa-envelope"></i></a>
            </nav>
        </header>
          <main role="main">
              <h1>VPC Peering with MongoDB Atlas and AWS in Terraform</h1>
              <article>
    <section class="header">
        Posted on 2018-12-26
    </section>
    <hr class="section-head">
    <section>
        <p>Lately I was doing a lot of “Infrastructure as Code” (IaC) in terraform at work. There was one application, which needed a MongoDB as the primary database. Sadly there is no managed MongoDB service in AWS so far. So there were two initial options for me:</p>
<ul>
<li>Host a self managed MongoDB in an EC2 instance</li>
<li>Use Amazons DynamoDB</li>
</ul>
<p>The main goal of the move into the cloud was to get away from all the administration tasks and have everything managed by the cloud hosters, so we have more time to get our actual problems solved. That’s why option one wasn’t really something we wantet to do. Option two wasn’t satisfactorily either, because we would go deep into the AWS rabbit hole. We wouldn’t have the chance to test the application locally and always needed a connection to an actual DynamoDB instance.</p>
<p>Update 10.02.2019: Actually there is a real alternative now called AWS DocumentDB. I didn’t look into it so far, so I don’t feel qualified to give any opinion on DocumentDB. But based on the MongoDB CEO MongoDB <a href="https://www.mongodb.com/blog/post/documents-are-everywhere">Atlas still has it’s right to exist</a>.</p>
<p>Luckily there was a third option called <a href="https://www.mongodb.com/cloud/atlas">MongoDB Atlas</a>. This is MongoDB Inc’s own take at DaaS (Database as a Service). They instanciate a replica set of three or more instances on one of the most common cloud providers (AWS, Google Cloud Platform or Azure) for you. Additionally you get backups, auto scaling, alerts and many more features. MongoDB Atlas is also following an API first approach like AWS, so the creation of MongoDB resources can be automated. Sadly there is no support for MongoDB Atlas in Terraform so far. Luckily Akshay Karle already took care of this problem and <a href="https://github.com/akshaykarle/terraform-provider-mongodbatlas">wrote a third party plugin for Terraform</a>.</p>
<p>In this blog post we create an infrastructure setup which consists of 2 AWS VPCs. In the first VPC our application will be hosted. This application will be able to communicate with the MongoDB replica set, hosted in another VPC by Atlas. Because we will host our MongoDB cluster in the same region, we can benefit of VPC peering. This means, that the application and MongoDB can communicate directly via local IPs between the two VPCs and no traffic goes out to the internet and back into the other VPC. This is more secure (Because you don’t need to care about securing the connection, because it’s local anyway) and also a lot of faster than sending every request and response over the internet. We will code everything in Terraform, so we are able to create our whole infrastructure with one <em>terraform apply</em> and are done with it. So let’s get started. Here is a rough overview of the most important pieces:</p>
<p><img src="../images/vpc_overview.svg" alt="VPC Overview" title="VPC Overview" /></p>
<p>As you can see we have our 2 VPCs. The left AWS VPC will be avaible in the CIDR block 172.16.0.0/16. The right VPC is the one created by Mongo Atlas and will be available under 10.0.0.0/21. Both VPCs are connected via VPC-Peering. Also the CIDR block of the AWS VPC is whitelisted in MongoDB Atlas. On top we have a route table, to route traffic between the two VPCs and to the internet via an internet gateway.</p>
<p>The following guide will be threefold. In the first part we create the route table, internet gateway and AWS VPC with it’s subnet. In part two the MongoDB part will be build. In the third part we will deploy a small application inside of an EC2 instance, which is able to interact with the MongoDB. The whole code for this guide can be found <a href="https://github.com/snowiow/mongodbatlas-example-app">here</a>.</p>
<h1 id="part-1-the-aws-vpc">Part 1: The AWS VPC</h1>
<p>In this part we will be creating the left side of our overview diagram. So basically this part:</p>
<p><img src="../images/aws_vpc.svg" alt="VPC Overview" title="AWS VPC" /></p>
<p>In my example project I have created a subfolder called <code>terraform</code> where all the infrastructure code can be found. First of all we load the AWS Provider. This is done in the <code>main.tf</code>:</p>
<pre class="hcl"><code>provider &quot;aws&quot; {
  region  = &quot;eu-central-1&quot;
  version = &quot;1.54&quot;
}</code></pre>
<p>Next we create the actual VPC. This is done in the <code>vpc.tf</code> file:</p>
<pre class="terraform"><code>resource &quot;aws_vpc&quot; &quot;this&quot; {
  cidr_block = &quot;172.16.0.0/16&quot;

  enable_dns_support   = true
  enable_dns_hostnames = true

  tags = {
    Name = &quot;vpc&quot;
  }
}</code></pre>
<p>As already stated in the beginning, we use the 172.16.0.0/16 CIDR block for the AWS VPC. We also enable DNS support and DNS hostnames inside of the VPC. With those two options we basically enable DNS discovery in the local VPC scope, which allows us to resolve the MongoDB cluster DNS to it’s private IP.</p>
<p>Now we create a subnet inside of the VPC:</p>
<pre class="hcl"><code>
resource &quot;aws_subnet&quot; &quot;this&quot; {
  cidr_block = &quot;176.16.0.0/16&quot;
  vpc_id     = &quot;${aws_vpc.this.id}&quot;

  map_public_ip_on_launch = true

  availability_zone = &quot;eu-central-1a&quot;

  tags = {
    Name = &quot;subnet1&quot;
  }
}</code></pre>
<p>Here we create one subnet, which takes all of the VPCs available addresses and is hosted in the availability zone eu-central-1a. Furthermore we want to give launched instances a public IP on startup to be able to download updates for the EC2 instance.</p>
<p>Next we put the internet gateway in front of the VPC, which is straight forward.</p>
<pre class="hcl"><code>resource &quot;aws_internet_gateway&quot; &quot;this&quot; {
  vpc_id = &quot;${aws_vpc.this.id}&quot;

  tags = {
    Name = &quot;internet-gateway&quot;
  }
}</code></pre>
<p>Finally we create the route table with a public route and associate our subnet to that route:</p>
<pre class="hcl"><code>resource &quot;aws_route_table&quot; &quot;this&quot; {
  vpc_id = &quot;${aws_vpc.this.id}&quot;

  tags = {
    Name = &quot;route-table-public&quot;
  }
}

resource &quot;aws_route&quot; &quot;this&quot; {
  route_table_id         = &quot;${aws_route_table.this.id}&quot;
  destination_cidr_block = &quot;0.0.0.0/0&quot;
  gateway_id             = &quot;${aws_internet_gateway.this.id}&quot;
}

resource &quot;aws_route_table_association&quot; &quot;this&quot; {
  route_table_id = &quot;${aws_route_table.this.id}&quot;
  subnet_id      = &quot;${aws_subnet.this.id}&quot;
}</code></pre>
<p>Basically what we are doing here is to allow to route traffic from our VPC to every address in the internet. This is mandatory, because we want to get responses from our web applications inside of the VPC.</p>
<p>That’s it for the first part. You are now able to run <code>terraform init &amp;&amp; terraform apply</code> to deploy the AWS VPC.</p>
<h1 id="part-2-mongodb-atlas">Part 2: MongoDB Atlas</h1>
<p>In this part we care about the right half of the diagram and create our MongoDB Cluster at Atlas, make a whitelist entry for our AWS VPC and create VPC Peering between the two VPCs:</p>
<p><img src="../images/mongodb_atlas.svg" alt="MongoDB Atlas" title="MongoDB Atlas" /></p>
<p>You can find the part 2 code in the <code>atlas.tf</code> file. We get started again, by adding a new provider to the <code>main.tf</code>:</p>
<pre class="hcl"><code>provider &quot;mongodbatlas&quot; {
  username = &quot;${var.username}&quot;
  api_key  = &quot;${var.api_key}&quot;
}</code></pre>
<p>To access your MongoDB Atlas account you need to pass your username and an API key. If you define them as variables like me, you can create a new file called <code>variables.tf</code> with the following content:</p>
<pre class="hcl"><code>variable &quot;username&quot; {
  type        = &quot;string&quot;
  description = &quot;The Username for the MongoDB Atlas Login&quot;
}

variable &quot;api_key&quot; {
  type        = &quot;string&quot;
  description = &quot;The API Key for the MongoDB Atlas Login&quot;
}</code></pre>
<p>this defines the variables used in the <code>main.tf</code> file. Next we create a <code>terraform.tfvars</code> in which we actually set the values of those variables:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode ini"><code class="sourceCode ini"><a class="sourceLine" id="cb8-1" title="1"><span class="dt">username </span><span class="ot">=</span><span class="st"> &quot;&lt;your_username&gt;&quot;</span></a>
<a class="sourceLine" id="cb8-2" title="2"><span class="dt">api_key </span><span class="ot">=</span><span class="st"> &quot;&lt;your_api_key&gt;&quot;</span></a></code></pre></div>
<p>Because we save trustworthy information in those variables, we don’t actually set them in a normal <code>*.tf</code> file. These files are checked into version control, so we don’t want to write those informations down in those files. Instead we use <code>*.tfvars</code> files, which aren’t checked into version control.</p>
<p>As said in the beginning, mongodb atlas isn’t supported by terraform officially. So we need to install the third party provider. To install the provider you need to have <a href="https://golang.org/">go</a> installed. With go installed you can get the package and symlink the executable to the plugin folder of terraform:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode sh"><code class="sourceCode bash"><a class="sourceLine" id="cb9-1" title="1"><span class="ex">go</span> get github.com/akshaykarle/terraform-provider-mongodbatlas</a>
<a class="sourceLine" id="cb9-2" title="2"><span class="fu">ln</span> -s <span class="va">$GOPATH</span>/bin/terraform-provider-mongodbatlas \</a>
<a class="sourceLine" id="cb9-3" title="3">      ~/.terraform.d/plugins/</a></code></pre></div>
<p>If you do a <code>terraform init</code> again, you are able to initialize the mongodbatlas provider as well.</p>
<p>With everything set up, we can start creating the MongoDB Atlas cluster. The first thing needed is a project (former known as groups). Projects are a sort of grouping to isolate different environments from each other or to configure different alert settings. For a full description head over to the <a href="https://docs.atlas.mongodb.com/tutorial/manage-projects/">official documentation</a>. We create a new file called <code>atlas.tf</code> for all our MongoDB Atlas resources. There we create a project first:</p>
<pre class="hcl"><code>resource &quot;mongodbatlas_project&quot; &quot;this&quot; {
  org_id = &quot;${var.org_id}&quot;
  name   = &quot;example-project&quot;
}</code></pre>
<p>To create a project resource we need an organisation id, which can be found in the settings tab:</p>
<p><img src="../images/mongodbatlas_organisation_id.png" alt="MongoDB Atlas Organisation ID" title="MongoDB Atlas Organisation ID" /></p>
<p>Because we use it as a variable in the <code>atlas.tf</code> file, we need to add it to our <code>terraform.tfvars</code></p>
<div class="sourceCode" id="cb11"><pre class="sourceCode ini"><code class="sourceCode ini"><a class="sourceLine" id="cb11-1" title="1"><span class="dt">...</span></a>
<a class="sourceLine" id="cb11-2" title="2"><span class="dt">org_id </span><span class="ot">=</span><span class="st"> &quot;&lt;your-organisation-id&gt;&quot;</span></a></code></pre></div>
<p>and <code>variables.tf</code></p>
<pre class="hcl"><code>variable &quot;org_id&quot; {
  type        = &quot;string&quot;
  description = &quot;The organisation id of the mongodb Login&quot;
}</code></pre>
<p>The next thing we need is a container. The container is the network of the cloud provider. In the example of AWS it would create a VPC.</p>
<pre class="hcl"><code>resource &quot;mongodbatlas_container&quot; &quot;this&quot; {
  group            = &quot;${mongodbatlas_project.this.id}&quot;
  atlas_cidr_block = &quot;10.0.0.0/21&quot;
  provider_name    = &quot;AWS&quot;
  region           = &quot;EU_CENTRAL_1&quot;
}</code></pre>
<p>Pay attention to the region names, because they are <a href="https://docs.atlas.mongodb.com/reference/api/clusters-create-one/">different than in AWS</a>.</p>
<p>Now we can create the cluster:</p>
<pre class="hcl"><code>resource &quot;mongodbatlas_cluster&quot; &quot;this&quot; {
  name                  = &quot;example&quot;
  group                 = &quot;${mongodbatlas_project.this.id}&quot;
  mongodb_major_version = &quot;4.0&quot;
  provider_name         = &quot;AWS&quot;
  region                = &quot;EU_CENTRAL_1&quot;
  size                  = &quot;M10&quot;
  disk_gb_enabled       = true
  backup                = false
  depends_on            = [&quot;mongodbatlas_container.this&quot;]
}</code></pre>
<p>Here we create a MongoDB Cluster in Version 4.0 in AWS with the same region as the container. M10 is the smallest non sandbox size available. It has 2GB of RAM, 10 GB of storage and 0.2 vCPUs. With <code>disk_gb_enabled</code> we allow the cluster to automatically scale up. Lastly the cluster should be created explicitly after the container.</p>
<p>Now we create a database user:</p>
<pre class="hcl"><code>resource &quot;mongodbatlas_database_user&quot; &quot;this&quot; {
  username = &quot;application-user&quot;
  password = &quot;application-pw&quot;
  database = &quot;admin&quot;
  group    = &quot;${mongodbatlas_project.this.id}&quot;

  roles {
    name     = &quot;readWrite
    database = &quot;app&quot;
  }
}</code></pre>
<p>This user will later be used by the application to access the MongoDB. He gets a username, password and a will be authenticated in the <code>admin</code> database. Those information can also be exported to the <code>terraform.tfvars</code> file, but for this example application I keep those information hardcoded in the <code>atlas.tf</code> file. The <code>admin</code> database is the default value for MongoDB Atlas. The user gets rights to read and write to the <code>app</code> database.</p>
<p>Next thing we do, is establishing the VPC peering connection:</p>
<pre class="hcl"><code>resource &quot;mongodbatlas_vpc_peering_connection&quot; &quot;this&quot; {
  group                  = &quot;${mongodbatlas_project.this.id}&quot;
  aws_account_id         = &quot;${var.aws_account_id}&quot;
  vpc_id                 = &quot;${aws_vpc.this.id}&quot;
  route_table_cidr_block = &quot;${aws_vpc.this.cidr_block}&quot;
  container_id           = &quot;${mongodbatlas_container.this.id}&quot;
}

resource &quot;aws_vpc_peering_connection_accepter&quot; &quot;this&quot; {
  vpc_peering_connection_id = &quot;${mongodbatlas_vpc_peering_connection.this.connection_id}&quot;
  auto_accept               = true
}</code></pre>
<p>First we create the MongoDB Atlas peering connection. The connection needs most of the stuff, we created before, like the AWS VPC to peer to and the container of our MongoDB cluster. Here we use another variable for the AWS account in which the destination VPC for the peering lies. This variable will be created analogous to those, we created earlier. The second thing is an acceptor, which should auto accept the peering requests for peerings with the MongoDB VPC.</p>
<p>Now we can also create an entry in our route table for the new MongoDB Atlas VPC, which allows traffic to be routed between those two VPCs properly:</p>
<pre class="hcl"><code>resource &quot;aws_route&quot; &quot;this&quot; {
  route_table_id            = &quot;${data.aws_route_table.this.id}&quot;
  destination_cidr_block    = &quot;${mongodbatlas_container.this.atlas_cidr_block}&quot;
  vpc_peering_connection_id = &quot;${mongodbatlas_vpc_peering_connection.this.connection_id}&quot;
}</code></pre>
<p>The last thing we have to do, is whitelist the AWS VPC CIDR Block in MongoDB Atlas, so that services inside of the AWS VPC are allowed to access the cluster.</p>
<pre class="hcl"><code>resource &quot;mongodbatlas_ip_whitelist&quot; &quot;this&quot; {
  group      = &quot;${mongodbatlas_project.this.id}&quot;
  cidr_block = &quot;${data.aws_vpc.this.cidr_block}&quot;
  comment    = &quot;Whitelist for the AWS VPC&quot;
}</code></pre>
<p>With this in place we are now able to create our MongoDB Atlas cluster. Again you can execute <code>terraform apply</code> to see the results.</p>
<h1 id="part-3-deploying-the-app">Part 3: Deploying the app</h1>
<p>In the last part we keep it as simple as possible. We will create a single EC2 instance, which will be provisioned to install a docker container with a small PHP application. The important part here, is that we now can give the DSN (Data Source Name) of the MongoDB to the app as an environment variable and the app is able to work with the MongoDB without any further manual interventions.</p>
<p>In reality there is a lot more to running a scalable infrastructure, like load balancing, autoscaling groups, launch templates and logging to name a few. But covering these topics in this post would crush the scope of this article, which is already pretty long at this point.</p>
<p>So without further ado let’s get startet by creating a <code>ec2.tf</code> file. First of all we create an AMI</p>
<pre class="hcl"><code>data &quot;aws_ami&quot; &quot;amazon_linux&quot; {
  most_recent = true
  owners      = [&quot;amazon&quot;]

  filter {
    name   = &quot;name&quot;
    values = [&quot;amzn2-ami-hvm-2.0.20181024-x86_64-gp2&quot;]
  }
}</code></pre>
<p>This defines an AMI (Amazon Machine Image) which is basically the operating system, the ec2 instance will be started with.</p>
<p>In this EC2 instance we will run a docker container. I created a repository in amazons own ECR for my container image. So I add the repository as a data source:</p>
<pre class="hcl"><code>data &quot;aws_ecr_repository&quot; &quot;example_app&quot; {
  name = &quot;snowiow/example-app&quot;
}</code></pre>
<p>Now we create a small shell script, which will be executed as soon as the instance is started.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb21-1" title="1"><span class="bu">echo</span> <span class="st">&quot;Update YUM&quot;</span></a>
<a class="sourceLine" id="cb21-2" title="2"><span class="fu">sudo</span> yum -y update</a>
<a class="sourceLine" id="cb21-3" title="3"></a>
<a class="sourceLine" id="cb21-4" title="4"><span class="bu">echo</span> <span class="st">&quot;Install Docker&quot;</span></a>
<a class="sourceLine" id="cb21-5" title="5"><span class="fu">sudo</span> yum install -y docker</a>
<a class="sourceLine" id="cb21-6" title="6"></a>
<a class="sourceLine" id="cb21-7" title="7"><span class="bu">echo</span> <span class="st">&quot;Start Docker&quot;</span></a>
<a class="sourceLine" id="cb21-8" title="8"><span class="fu">sudo</span> service docker start</a>
<a class="sourceLine" id="cb21-9" title="9"></a>
<a class="sourceLine" id="cb21-10" title="10"><span class="bu">echo</span> <span class="st">&quot;Login to ECR (your Docker Registry)&quot;</span></a>
<a class="sourceLine" id="cb21-11" title="11"><span class="va">$(</span><span class="ex">aws</span> ecr get-login --no-include-email --region eu-central-1<span class="va">)</span></a>
<a class="sourceLine" id="cb21-12" title="12"></a>
<a class="sourceLine" id="cb21-13" title="13"><span class="bu">echo</span> <span class="st">&quot;Start docker container&quot;</span></a>
<a class="sourceLine" id="cb21-14" title="14"><span class="ex">docker</span> run \</a>
<a class="sourceLine" id="cb21-15" title="15">  -p 80:80 \</a>
<a class="sourceLine" id="cb21-16" title="16">  --env <span class="st">&quot;MONGO_DSN=</span><span class="va">${mongodb_dsn}</span><span class="st">&quot;</span> \</a>
<a class="sourceLine" id="cb21-17" title="17">  --env <span class="st">&quot;MONGO_DB=app&quot;</span> \</a>
<a class="sourceLine" id="cb21-18" title="18">  <span class="va">${container_img_url}</span></a></code></pre></div>
<p>Basically this installs docker, downloads the image from the container repository URL, where I uploaded it and executes it. As environment variables I give the container the DSN of the MongoDB cluster, the container repository URL and the database, which will always be called app, so it is hardcoded here. The DSN and repository URL values are interpolated, so we need a way to fill in the actual values. We do this by creating a template file as a data source in terraform:</p>
<pre class="hcl"><code>data &quot;template_file&quot; &quot;user_data&quot; {
  template = &quot;${file(&quot;user_data.sh&quot;)}&quot;

  vars {
    mongodb_dsn    = &quot;mongodb://${mongodbatlas_database_user.this.username}:${mongodbatlas_database_user.this.password}@${substr(mongodbatlas_cluster.this.mongo_uri_with_options, 10, -1)}&quot;
    docker_img_url = &quot;${data.aws_ecr_repository.example_app.repository_url}&quot;
  }
}</code></pre>
<p>Here we have to do some string manipulations to get the username and password in the DSN as well, otherwise the app wouldn’t be able to login to the cluster.</p>
<p>The last thing we need before creating our instances, is an IAM Role. The EC2 instance we launch will assume this role and will have the rights provided by this role. First of all we need a policy document, which says, that the EC2 instance is allowed to assume a role</p>
<pre class="hcl"><code>data &quot;aws_iam_policy_document&quot; &quot;assume&quot; {
  statement {
    sid     = &quot;AllowAssumeByEC2&quot;
    effect  = &quot;Allow&quot;
    actions = [&quot;sts:AssumeRole&quot;]

    principals {
      type        = &quot;Service&quot;
      identifiers = [&quot;ec2.amazonaws.com&quot;]
    }
  }
}</code></pre>
<p>Now we can create the role itself</p>
<pre class="hcl"><code>resource &quot;aws_iam_role&quot; &quot;example_app&quot; {
  name               = &quot;example-app-iam-role&quot;
  assume_role_policy = &quot;${data.aws_iam_policy_document.assume.json}&quot;
}</code></pre>
<p>Because our EC2 instance needs to download a Docker Image from the ECR, we need to give the right to the role first. Therefore we create another policy document:</p>
<pre class="hcl"><code>data &quot;aws_iam_policy_document&quot; &quot;ecr&quot; {
  statement {
    sid    = &quot;AllowECRAuthorization&quot;
    effect = &quot;Allow&quot;

    actions = [
      &quot;ecr:GetAuthorizationToken&quot;,
    ]

    resources = [&quot;*&quot;]
  }

  statement {
    sid    = &quot;AllowECRDownload&quot;
    effect = &quot;Allow&quot;

    actions = [
      &quot;ecr:GetDownloadUrlForLayer&quot;,
      &quot;ecr:BatchGetImage&quot;,
      &quot;ecr:BatchCheckLayerAvailability&quot;,
    ]

    resources = [&quot;${data.aws_ecr_repository.example_app.arn}&quot;]
  }
}</code></pre>
<p>This policy consists of two statements. The first one allows an action to authorize ourselfs with the ECR, which we do in the <code>user_data.sh</code> script with this statement:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb26-1" title="1"><span class="va">$(</span><span class="ex">aws</span> ecr get-login --no-include-email --region eu-central-1<span class="va">)</span></a></code></pre></div>
<p>As a resource <code>*</code> was chosen, because the <code>ecr:GetAuthorizationToken</code> action is global and can’t be restricted to a specific resource. The second statement allows the download of the image. Here we defined the ARN of the specific repository, we want to pull from, because we don’t want to allow our EC2 instance to pull from every repository we have in our account.</p>
<p>With this policy document, we can create an actual policy from it, which will be attached to our <code>example-app-role</code>:</p>
<pre class="hcl"><code>resource &quot;aws_iam_policy&quot; &quot;ecr&quot; {
  name        = &quot;ExampleAppECRAccess&quot;
  description = &quot;Gives right to get an ECR authorization token and pull images&quot;
  policy      = &quot;${data.aws_iam_policy_document.ecr.json}&quot;
}

resource &quot;aws_iam_role_policy_attachment&quot; &quot;ecr&quot; {
  role       = &quot;${aws_iam_role.example_app.name}&quot;
  policy_arn = &quot;${aws_iam_policy.ecr.arn}&quot;
}</code></pre>
<p>Because we can’t attach a role directly to an EC2 instance, we need an instance profile:</p>
<pre class="hcl"><code>resource &quot;aws_iam_instance_profile&quot; &quot;this&quot; {
  name = &quot;example-app-instance-profile&quot;
  role = &quot;${aws_iam_role.example_app.name}&quot;
}</code></pre>
<p>Finally we create the EC2 instance itself and output the IP address, where it is reachable afterwards. We also need a security group for the instance, which basically tells who will be able to access the instance and to who the instance is allowed to respond to. In this example we allow traffic from any IP over port 80, because this is the port where the website is hosted:</p>
<pre class="hcl"><code>resource &quot;aws_security_group&quot; &quot;this&quot; {
  name   = &quot;sg&quot;
  vpc_id = &quot;${aws_vpc.this.id}&quot;

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = &quot;tcp&quot;
    cidr_blocks = [&quot;0.0.0.0/0&quot;]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = &quot;-1&quot;
    cidr_blocks = [&quot;0.0.0.0/0&quot;]
  }
}

resource &quot;aws_instance&quot; &quot;this&quot; {
  ami                  = &quot;${data.aws_ami.amazon_linux.id}&quot;
  instance_type        = &quot;t2.micro&quot;
  subnet_id            = &quot;${aws_subnet.this.id}&quot;
  user_data            = &quot;${data.template_file.user_data.rendered}&quot;
  iam_instance_profile = &quot;${aws_iam_instance_profile.this.name}&quot;
  security_groups      = [&quot;${aws_security_group.this.id}&quot;]
}

output &quot;example_app&quot; {
  value = &quot;${aws_instance.this.public_ip}&quot;
}</code></pre>
<p>This is it! If you execute <code>terraform apply</code> again, you will see the public IP of the EC2 instance. To see if the connection to the MongoDB is working we can play around with the example application.</p>
<p>The example application is a small website with two routes. Both use GET parameters. The first route looks like this:</p>
<pre><code>/insert/{firstname}/{lastname}</code></pre>
<p>This route will insert a user with a first and lastname into the MongoDB. For the example of will willson we combine the returned IP of the terraform script with the route like this</p>
<pre><code>&lt;ec2ip&gt;/insert/will/willson</code></pre>
<p>the response would look like this:</p>
<p><img src="../images/insert_will_willson.png" alt="Insert Will Willson" title="Insert Will Willson" /></p>
<p>To really see if the user was inserted into MongoDB, we use the second route, which looks like this:</p>
<pre><code>/{lastname}</code></pre>
<p>Basically if we insert the lastname as the only part of the route, the example application searches for users with this lastname and will print them. For the example of willson the URL would look like this:</p>
<pre><code>&lt;ec2ip&gt;/willson</code></pre>
<p>and we get this response</p>
<p><img src="../images/will_willson_is_here.png" alt="Will Willson is here" title="Will Willson is here" /></p>
<p>This application is pretty basic but works as a proof of concept of our infrastructure.</p>
<p>So there you have it. A VPC peering connection between MongoDB Atlas and our own AWS VPC, which is applicable in a single command, thanks to terraform. I created this post because there are many useful resources scattered around about this topic, but there is no single resource which combines them all together. So I hope this is of help for anybody who is trying to achieve something similar.</p>
    </section>
</article>

          </main>
          <footer>
            <nav>
                <a href="../impressum.html" title="Impressum">Impressum</a>
                <a href="../datenschutz.html" title="Datenschutz">Datenschutz</a>
            </nav>
          </footer>
    </body>
</html>
